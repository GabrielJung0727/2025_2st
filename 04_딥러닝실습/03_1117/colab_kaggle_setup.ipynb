{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb09507",
   "metadata": {},
   "source": [
    "# CLIP 기반 객체 분석 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef30a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install matplotlib seaborn scikit-learn kaggle tqdm open_clip_torch >/dev/null 2>&1 || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f87a9351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52c93b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "src = Path('api/kaggle.json')\n",
    "target = Path('~/.kaggle/kaggle.json').expanduser()\n",
    "target.parent.mkdir(parents=True, exist_ok=True)\n",
    "if src.exists():\n",
    "    shutil.copyfile(src, target)\n",
    "    os.chmod(target, 0o600)\n",
    "else:\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "        if 'kaggle.json' not in uploaded:\n",
    "            raise RuntimeError('kaggle.json missing')\n",
    "        with open(target, 'wb') as f:\n",
    "            f.write(uploaded['kaggle.json'])\n",
    "        os.chmod(target, 0o600)\n",
    "    except Exception:\n",
    "        print('skip kaggle token upload (not on Colab)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95058a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DRIVE = False\n",
    "DRIVE_DIR = '/content/drive/MyDrive/school/딥러닝실습/1117/data'\n",
    "if USE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.makedirs(DRIVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "941c5116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gabriel/Documents/school/딥러닝실습/1117/data\n",
      "/Users/gabriel/Documents/school/딥러닝실습/1117/data/images\n",
      "/Users/gabriel/Documents/school/딥러닝실습/1117/data/annotations.csv\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(DRIVE_DIR) if USE_DRIVE else Path('data').resolve()\n",
    "IMAGES_DIR = DATA_DIR / 'images'\n",
    "ANNO_CSV = DATA_DIR / 'annotations.csv'\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "print(DATA_DIR)\n",
    "print(IMAGES_DIR)\n",
    "print(ANNO_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cd9dc3",
   "metadata": {},
   "source": [
    "## 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e931830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수: 3425\n",
      "클래스: ['Airplane', 'Truncated_airplane']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "      <th>bbox</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg</td>\n",
       "      <td>Airplane</td>\n",
       "      <td>[135.0, 522.0, 245.0, 600.0]</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg</td>\n",
       "      <td>Airplane</td>\n",
       "      <td>[1025.0, 284.0, 1125.0, 384.0]</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg</td>\n",
       "      <td>Airplane</td>\n",
       "      <td>[1058.0, 1503.0, 1130.0, 1568.0]</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg</td>\n",
       "      <td>Airplane</td>\n",
       "      <td>[813.0, 1518.0, 885.0, 1604.0]</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg</td>\n",
       "      <td>Airplane</td>\n",
       "      <td>[594.0, 938.0, 657.0, 1012.0]</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   filename     label  \\\n",
       "0  4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg  Airplane   \n",
       "1  4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg  Airplane   \n",
       "2  4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg  Airplane   \n",
       "3  4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg  Airplane   \n",
       "4  4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg  Airplane   \n",
       "\n",
       "                               bbox split  \n",
       "0      [135.0, 522.0, 245.0, 600.0]  test  \n",
       "1    [1025.0, 284.0, 1125.0, 384.0]  test  \n",
       "2  [1058.0, 1503.0, 1130.0, 1568.0]  test  \n",
       "3    [813.0, 1518.0, 885.0, 1604.0]  test  \n",
       "4     [594.0, 938.0, 657.0, 1012.0]  test  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(ANNO_CSV, comment='#')\n",
    "aliases = {'image_id': 'filename', 'image_path': 'filename', 'file': 'filename', 'filepath': 'filename', 'class': 'label', 'category': 'label'}\n",
    "for old, new in aliases.items():\n",
    "    if old in df.columns and new not in df.columns:\n",
    "        df[new] = df[old]\n",
    "required = {'filename', 'geometry', 'label'}\n",
    "if not required.issubset(df.columns):\n",
    "    raise ValueError('annotations.csv must include filename/image_id, geometry, class/label columns')\n",
    "\n",
    "df['filename'] = df['filename'].astype(str)\n",
    "df['label'] = df['label'].astype(str)\n",
    "\n",
    "def resolve_path(value):\n",
    "    p = Path(value)\n",
    "    if p.is_absolute():\n",
    "        return p\n",
    "    if p.parts and p.parts[0] == 'images':\n",
    "        return DATA_DIR / p\n",
    "    return IMAGES_DIR / p\n",
    "\n",
    "df['abs_path'] = df['filename'].apply(resolve_path)\n",
    "exists_mask = df['abs_path'].map(Path.exists)\n",
    "if not exists_mask.all():\n",
    "    missing = df.loc[~exists_mask, 'filename'].unique().tolist()[:5]\n",
    "    print('dropping entries with missing files:', missing)\n",
    "    df = df[exists_mask]\n",
    "\n",
    "def to_box(value):\n",
    "    pts = ast.literal_eval(value)\n",
    "    xs = [float(pt[0]) for pt in pts]\n",
    "    ys = [float(pt[1]) for pt in pts]\n",
    "    return [min(xs), min(ys), max(xs), max(ys)]\n",
    "\n",
    "df['bbox'] = df['geometry'].apply(to_box)\n",
    "\n",
    "if 'split' not in df.columns:\n",
    "    files = df['filename'].unique()\n",
    "    train_ids, tmp_ids = train_test_split(files, test_size=0.3, random_state=42)\n",
    "    val_ids, test_ids = train_test_split(tmp_ids, test_size=0.5, random_state=42)\n",
    "    split_map = {fid: 'train' for fid in train_ids}\n",
    "    split_map.update({fid: 'val' for fid in val_ids})\n",
    "    split_map.update({fid: 'test' for fid in test_ids})\n",
    "    df['split'] = df['filename'].map(split_map)\n",
    "\n",
    "df['split'] = df['split'].str.lower()\n",
    "class_names = sorted(df['label'].unique())\n",
    "label_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "df['label_id'] = df['label'].map(label_to_idx)\n",
    "print('총 샘플 수:', len(df))\n",
    "print('클래스:', class_names)\n",
    "display(df[['filename', 'label', 'bbox', 'split']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3ed0b",
   "metadata": {},
   "source": [
    "## Dataset / DataLoader 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efd090bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용할 split: val 샘플 수: 528\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f71e9932727403e9b85126004cc27a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "528"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT_TO_USE = 'val'  # 'train' / 'val' / 'test' 중 선택\n",
    "MAX_SAMPLES_PER_CLASS = None  # 정수로 설정하면 클래스별 최대 샘플 수 제한\n",
    "subset = df[df['split'] == SPLIT_TO_USE].copy()\n",
    "if MAX_SAMPLES_PER_CLASS:\n",
    "    subset = subset.groupby('label').head(MAX_SAMPLES_PER_CLASS).reset_index(drop=True)\n",
    "subset = subset.reset_index(drop=True)\n",
    "print('사용할 split:', SPLIT_TO_USE, '샘플 수:', len(subset))\n",
    "\n",
    "class ClipRegionDataset(Dataset):\n",
    "    def __init__(self, frame, preprocess):\n",
    "        self.frame = frame.reset_index(drop=True)\n",
    "        self.preprocess = preprocess\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.frame.iloc[idx]\n",
    "        image = Image.open(row['abs_path']).convert('RGB')\n",
    "        width, height = image.size\n",
    "        x0, y0, x1, y1 = row['bbox']\n",
    "        x0 = max(0, min(width, x0))\n",
    "        y0 = max(0, min(height, y0))\n",
    "        x1 = max(0, min(width, x1))\n",
    "        y1 = max(0, min(height, y1))\n",
    "        if x1 <= x0 or y1 <= y0:\n",
    "            crop = image\n",
    "        else:\n",
    "            crop = image.crop((x0, y0, x1, y1))\n",
    "        tensor = self.preprocess(crop)\n",
    "        return tensor, row['label_id'], row['label']\n",
    "\n",
    "MODEL_NAME = 'ViT-B-32'\n",
    "PRETRAINED = 'laion2b_s34b_b79k'\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=PRETRAINED)\n",
    "model = model.to(device)\n",
    "text_prompts = [f\"a photo of a {name.replace('_', ' ')}\" for name in class_names]\n",
    "text_tokens = open_clip.tokenize(text_prompts).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "NUM_WORKERS = 0  # 멀티프로세싱 호환 이슈를 피하기 위해 기본값 0 (Colab GPU라면 수동으로 늘려도 됨)\n",
    "dataset = ClipRegionDataset(subset, preprocess)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ee2e2",
   "metadata": {},
   "source": [
    "## CLIP 추론 및 지표 산출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c0956",
   "metadata": {},
   "source": [
    "## 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ac0c677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)    \n",
      "exitcode = _main(fd, parent_sentinel)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'ClipRegionDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'ClipRegionDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  0%|          | 0/17 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 21857) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/딥러닝실습/1117/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1274\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    112\u001b[39m timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:1135\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1135\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/딥러닝실습/1117/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[39m, in \u001b[36m_set_SIGCHLD_handler.<locals>.handler\u001b[39m\u001b[34m(signum, frame)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandler\u001b[39m(signum, frame):\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid 21857) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     23\u001b[39m             image_embeds.extend(image_features.cpu().tolist())\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     25\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mpreds\u001b[39m\u001b[33m'\u001b[39m: np.array(all_preds),\n\u001b[32m     26\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m: np.array(all_labels),\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m         \u001b[33m'\u001b[39m\u001b[33membeds\u001b[39m\u001b[33m'\u001b[39m: np.array(image_embeds)\n\u001b[32m     31\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m outputs = \u001b[43mevaluate_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m preds = outputs[\u001b[33m'\u001b[39m\u001b[33mpreds\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     35\u001b[39m labels = outputs[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mevaluate_clip\u001b[39m\u001b[34m(model, loader, text_features)\u001b[39m\n\u001b[32m      8\u001b[39m image_embeds = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/딥러닝실습/1117/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/딥러닝실습/1117/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/딥러닝실습/1117/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/딥러닝실습/1117/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1444\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1441\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1446\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/딥러닝실습/1117/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1288\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1287\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1289\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1290\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1292\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 21857) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "def evaluate_clip(model, loader, text_features):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_label_names = []\n",
    "    confidences = []\n",
    "    raw_probs = []\n",
    "    image_embeds = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels, label_names in tqdm(loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            logits = (model.logit_scale.exp() * image_features @ text_features.T)\n",
    "            probs = logits.softmax(dim=-1)\n",
    "            preds = probs.argmax(dim=-1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            all_label_names.extend(label_names)\n",
    "            confidences.extend(probs.max(dim=-1).values.cpu().tolist())\n",
    "            raw_probs.extend(probs.cpu().tolist())\n",
    "            image_embeds.extend(image_features.cpu().tolist())\n",
    "    return {\n",
    "        'preds': np.array(all_preds),\n",
    "        'labels': np.array(all_labels),\n",
    "        'label_names': np.array(all_label_names),\n",
    "        'confidences': np.array(confidences),\n",
    "        'probs': np.array(raw_probs),\n",
    "        'embeds': np.array(image_embeds)\n",
    "    }\n",
    "\n",
    "if len(dataset) == 0:\n",
    "    raise ValueError('선택한 split에 샘플이 없습니다. SPLIT_TO_USE 또는 MAX_SAMPLES_PER_CLASS를 조정하세요.')\n",
    "\n",
    "outputs = evaluate_clip(model, loader, text_features)\n",
    "preds = outputs['preds']\n",
    "labels = outputs['labels']\n",
    "confidences = outputs['confidences']\n",
    "embeds = outputs['embeds']\n",
    "print('samples evaluated:', len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb6b90a",
   "metadata": {},
   "source": [
    "## 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ddd3a",
   "metadata": {},
   "source": [
    "## 분류 리포트 및 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6882a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(labels) == 0:\n",
    "    raise ValueError('No samples available for the selected split. Adjust SPLIT_TO_USE or MAX_SAMPLES_PER_CLASS.')\n",
    "idx_to_label = {idx: name for name, idx in label_to_idx.items()}\n",
    "class_names_ordered = [idx_to_label[i] for i in range(len(class_names))]\n",
    "report = classification_report(labels, preds, target_names=class_names_ordered, zero_division=0, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "display(report_df)\n",
    "acc = (preds == labels).mean()\n",
    "print('overall accuracy:', round(acc, 4))\n",
    "results_df = pd.DataFrame({\n",
    "    'true_idx': labels,\n",
    "    'pred_idx': preds,\n",
    "    'true_label': [idx_to_label[i] for i in labels],\n",
    "    'pred_label': [idx_to_label[i] for i in preds],\n",
    "    'confidence': confidences\n",
    "})\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f58237",
   "metadata": {},
   "source": [
    "## 그래프 1: 클래스별 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a645b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class = results_df.groupby('true_label').apply(lambda g: (g['true_label'] == g['pred_label']).mean()).reset_index(name='accuracy')\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(per_class['true_label'], per_class['accuracy'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Per-class CLIP Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a17093",
   "metadata": {},
   "source": [
    "## 그래프 2: 혼동 행렬 히트맵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6433fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(labels, preds, labels=list(range(len(class_names))))\n",
    "cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_norm, xticklabels=class_names_ordered, yticklabels=class_names_ordered, cmap='Blues', annot=True, fmt='.2f')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d6610",
   "metadata": {},
   "source": [
    "## 그래프 3: 신뢰도 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.boxplot(data=results_df, x='true_label', y='confidence')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Confidence Distribution per Class (True Label 기준)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca5737c",
   "metadata": {},
   "source": [
    "## 그래프 4: 임베딩 PCA 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count = min(len(embeds), 2000)\n",
    "embed_subset = embeds[:sample_count]\n",
    "label_subset = labels[:sample_count]\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "proj = pca.fit_transform(embed_subset)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(proj[:, 0], proj[:, 1], c=label_subset, cmap='tab10', alpha=0.6)\n",
    "plt.title('CLIP Image Embeddings (PCA)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87741cac",
   "metadata": {},
   "source": [
    "## 샘플 크롭 예측 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdad933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(crop_image):\n",
    "    with torch.no_grad():\n",
    "        tensor = preprocess(crop_image).unsqueeze(0).to(device)\n",
    "        feat = model.encode_image(tensor)\n",
    "        feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "        logits = (model.logit_scale.exp() * feat @ text_features.T)\n",
    "        probs = logits.softmax(dim=-1).cpu().numpy()[0]\n",
    "        pred_idx = probs.argmax()\n",
    "        return pred_idx, probs[pred_idx]\n",
    "\n",
    "if len(subset) == 0:\n",
    "    print('subset is empty, skip visualization')\n",
    "else:\n",
    "    samples = subset.sample(n=min(4, len(subset)), random_state=0)\n",
    "    fig, axes = plt.subplots(1, len(samples), figsize=(14, 4))\n",
    "    if len(samples) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, (_, row) in zip(axes, samples.iterrows()):\n",
    "        image = Image.open(row['abs_path']).convert('RGB')\n",
    "        x0, y0, x1, y1 = row['bbox']\n",
    "        crop = image.crop((x0, y0, x1, y1))\n",
    "        pred_idx, conf = predict_single(crop)\n",
    "        pred_label = idx_to_label[pred_idx]\n",
    "        ax.imshow(crop)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"true: {row['label']}\n",
    "pred: {pred_label} ({conf:.2f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4717b2",
   "metadata": {},
   "source": [
    "## Kaggle 다운로드 (옵션)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'dataset'\n",
    "TARGET = 'zynicide/wine-reviews'\n",
    "SELECT_FILES = []\n",
    "UNZIP = True\n",
    "OVERWRITE = True\n",
    "print(MODE, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle import KaggleApi\n",
    "from requests.exceptions import HTTPError\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "def download_dataset(slug, dest, files, unzip, force):\n",
    "    if files:\n",
    "        for f in files:\n",
    "            api.dataset_download_file(slug, f, path=dest, force=force, quiet=False)\n",
    "    else:\n",
    "        api.dataset_download_files(slug, path=dest, unzip=unzip, force=force, quiet=False)\n",
    "def download_competition(slug, dest, files, unzip, force):\n",
    "    if files:\n",
    "        for f in files:\n",
    "            api.competition_download_file(slug, f, path=dest, force=force, quiet=False)\n",
    "    else:\n",
    "        api.competition_download_files(slug, path=dest, quiet=False)\n",
    "    if unzip:\n",
    "        for z in Path(dest).glob('*.zip'):\n",
    "            import subprocess\n",
    "            subprocess.run(['unzip', '-o', str(z), '-d', dest], check=False)\n",
    "try:\n",
    "    if MODE == 'dataset':\n",
    "        download_dataset(TARGET, str(DATA_DIR), SELECT_FILES, UNZIP, OVERWRITE)\n",
    "    elif MODE == 'competition':\n",
    "        download_competition(TARGET, str(DATA_DIR), SELECT_FILES, UNZIP, OVERWRITE)\n",
    "    else:\n",
    "        raise ValueError('invalid mode')\n",
    "    print('done')\n",
    "except HTTPError as e:\n",
    "    print('error', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
